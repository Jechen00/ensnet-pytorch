{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b253c2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T07:11:04.819129Z",
     "start_time": "2025-04-14T07:11:04.814815Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6e0b2ebd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T07:47:45.334990Z",
     "start_time": "2025-04-14T07:47:45.307491Z"
    }
   },
   "outputs": [],
   "source": [
    "class DropConnectLinear(nn.Linear):\n",
    "    '''\n",
    "    Implements a drop connection before a linear layer. \n",
    "    This is based on code from torchnlp: \n",
    "        https://pytorchnlp.readthedocs.io/en/latest/_modules/torchnlp/nn/weight_drop.html\n",
    "    Note: The torchnlp implementation may not follow the original drop connect paper. \n",
    "          For one, it uses inverted dropout.\n",
    "    Original Paper: https://proceedings.mlr.press/v28/wan13.html\n",
    "    \n",
    "    Args:\n",
    "        in_features (int): Number of features in the input.\n",
    "        out_features (int): Number of features in the output.\n",
    "        drop_prob (float): Probability of a weight being dropped during training.\n",
    "    '''\n",
    "    def __init__(self, in_features: int, out_features: int, drop_prob: float = 0.5, **kwargs):\n",
    "        super().__init__(in_features, out_features, **kwargs)\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Drop random weights during training\n",
    "        drop_weights = F.dropout(self.weight, p = self.drop_prob, training = self.training)\n",
    "        \n",
    "        return F.linear(X, drop_weights, self.bias)\n",
    "\n",
    "class ConvBNDrop(nn.Module):\n",
    "    def __init__(self, \n",
    "                 out_channels: int, \n",
    "                 kernel_size: int = 3 , \n",
    "                 drop_prob: float = 0.35, \n",
    "                 pre_dropout: bool = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = [nn.LazyConv2d(out_channels, kernel_size), nn.ReLU(),\n",
    "                  nn.BatchNorm2d(out_channels),\n",
    "                  nn.Dropout(drop_prob)]\n",
    "        \n",
    "        if pre_dropout:\n",
    "            # Move the last element (Dropout) to the front\n",
    "            layers = [layers[-1]] + layers[:-1]\n",
    "        \n",
    "        self.conv_bn_drop = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.conv_bn_drop(X)\n",
    "\n",
    "class EnsNetBaseCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn_body = nn.Sequential()\n",
    "        \n",
    "        self.cnn_body.add_module(\n",
    "            'cnn_block_1',\n",
    "            nn.Sequential(\n",
    "                ConvBNDrop(64, kernel_size = 3, drop_prob = 0.35),\n",
    "                ConvBNDrop(128, kernel_size = 3, drop_prob = 0.35),\n",
    "\n",
    "                nn.LazyConv2d(256, kernel_size = 3), nn.ReLU(),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.MaxPool2d(kernel_size = 2)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.cnn_body.add_module(\n",
    "            'cnn_block_2',\n",
    "            nn.Sequential(\n",
    "                ConvBNDrop(512, kernel_size = 3, drop_prob = 0.35, pre_dropout = True),\n",
    "                ConvBNDrop(1024, kernel_size = 3, drop_prob = 0.35, pre_dropout = True),\n",
    "                ConvBNDrop(2000, kernel_size = 3, drop_prob = 0.35, pre_dropout = True),\n",
    "\n",
    "                nn.MaxPool2d(kernel_size = 2),\n",
    "                nn.Dropout(0.35)\n",
    "            )  \n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(512), nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.5),\n",
    "            DropConnectLinear(512, 512, drop_prob = 0.5), nn.ReLU(),\n",
    "            nn.LazyLinear(10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        final_feature_maps = self.cnn_body(X)\n",
    "        logits = self.classifier(final_feature_maps)\n",
    "        \n",
    "        return logits, final_feature_maps\n",
    "\n",
    "class EnsNetFCSN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.subnet = nn.Sequential(\n",
    "            nn.LazyLinear(512), nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.5),\n",
    "            DropConnectLinear(512, 512, drop_prob = 0.5), nn.ReLU(),\n",
    "            nn.LazyLinear(10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.subnet(X)\n",
    "        \n",
    "class EnsNet(nn.Module):\n",
    "    def __init__(self, num_subnets):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_subnets = num_subnets\n",
    "        \n",
    "        # Create base CNN to extract features before division\n",
    "        self.base_cnn = EnsNetBaseCNN()\n",
    "        \n",
    "        self.subnets = nn.ModuleList([\n",
    "            EnsNetFCSN() for _ in range(num_subnets)\n",
    "        ])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Get logits from the base CNN and subnets\n",
    "        cnn_logits, subnet_logits, _ = self.forward(X)\n",
    "        all_logits = [cnn_logits] + subnet_logits # num_voters = num_subnets + 1\n",
    "\n",
    "        # Get predicted classes from the base CNN and each subnet\n",
    "        pred_classes = torch.stack(all_logits, dim = 0).argmax(dim = -1) # Shape: (num_voters, batch_size)\n",
    " \n",
    "        # Get majority vote among predicted classes\n",
    "        return torch.mode(pred_classes, dim = 0).values # Mode along voter dimension\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Get logits and feature maps from base CNN\n",
    "        cnn_logits, cnn_feat_maps = self.base_cnn(X)\n",
    "\n",
    "        # Divide the feature maps into distinct chunks\n",
    "            # If cnn_feat_maps[1] isn't divisible by num_subnets, the last chunk will have less channels\n",
    "        div_feat_maps = torch.chunk(cnn_feat_maps, chunks = self.num_subnets, dim = 1)\n",
    "\n",
    "        # Get logits from subnetworks\n",
    "        subnet_logits = []\n",
    "        for feat_map, subnet in zip(div_feat_maps, self.subnets):\n",
    "            feat_map = torch.flatten(feat_map, start_dim = 1) # Flatten feature map\n",
    "            subnet_logits.append(subnet(feat_map))\n",
    "\n",
    "        return cnn_logits, subnet_logits, cnn_feat_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "de145890",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T07:47:49.792774Z",
     "start_time": "2025-04-14T07:47:49.252160Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type (var_name))                                 Input Shape          Output Shape         Param #\n",
       "===================================================================================================================\n",
       "EnsNet (EnsNet)                                         [64, 1, 28, 28]      [64, 10]             --\n",
       "├─EnsNetBaseCNN (base_cnn)                              [64, 1, 28, 28]      [64, 10]             --\n",
       "│    └─Sequential (cnn_body)                            [64, 1, 28, 28]      [64, 2000, 2, 2]     --\n",
       "│    │    └─Sequential (cnn_block_1)                    [64, 1, 28, 28]      [64, 256, 11, 11]    370,560\n",
       "│    │    └─Sequential (cnn_block_2)                    [64, 256, 11, 11]    [64, 2000, 2, 2]     24,340,848\n",
       "│    └─Sequential (classifier)                          [64, 2000, 2, 2]     [64, 10]             --\n",
       "│    │    └─Flatten (0)                                 [64, 2000, 2, 2]     [64, 8000]           --\n",
       "│    │    └─Linear (1)                                  [64, 8000]           [64, 512]            4,096,512\n",
       "│    │    └─ReLU (2)                                    [64, 512]            [64, 512]            --\n",
       "│    │    └─BatchNorm1d (3)                             [64, 512]            [64, 512]            1,024\n",
       "│    │    └─Dropout (4)                                 [64, 512]            [64, 512]            --\n",
       "│    │    └─DropConnectLinear (5)                       [64, 512]            [64, 512]            262,656\n",
       "│    │    └─ReLU (6)                                    [64, 512]            [64, 512]            --\n",
       "│    │    └─Linear (7)                                  [64, 512]            [64, 10]             5,130\n",
       "├─ModuleList (subnets)                                  --                   --                   --\n",
       "│    └─EnsNetFCSN (0)                                   [64, 800]            [64, 10]             --\n",
       "│    │    └─Sequential (subnet)                         [64, 800]            [64, 10]             678,922\n",
       "│    └─EnsNetFCSN (1)                                   [64, 800]            [64, 10]             --\n",
       "│    │    └─Sequential (subnet)                         [64, 800]            [64, 10]             678,922\n",
       "│    └─EnsNetFCSN (2)                                   [64, 800]            [64, 10]             --\n",
       "│    │    └─Sequential (subnet)                         [64, 800]            [64, 10]             678,922\n",
       "│    └─EnsNetFCSN (3)                                   [64, 800]            [64, 10]             --\n",
       "│    │    └─Sequential (subnet)                         [64, 800]            [64, 10]             678,922\n",
       "│    └─EnsNetFCSN (4)                                   [64, 800]            [64, 10]             --\n",
       "│    │    └─Sequential (subnet)                         [64, 800]            [64, 10]             678,922\n",
       "│    └─EnsNetFCSN (5)                                   [64, 800]            [64, 10]             --\n",
       "│    │    └─Sequential (subnet)                         [64, 800]            [64, 10]             678,922\n",
       "│    └─EnsNetFCSN (6)                                   [64, 800]            [64, 10]             --\n",
       "│    │    └─Sequential (subnet)                         [64, 800]            [64, 10]             678,922\n",
       "│    └─EnsNetFCSN (7)                                   [64, 800]            [64, 10]             --\n",
       "│    │    └─Sequential (subnet)                         [64, 800]            [64, 10]             678,922\n",
       "│    └─EnsNetFCSN (8)                                   [64, 800]            [64, 10]             --\n",
       "│    │    └─Sequential (subnet)                         [64, 800]            [64, 10]             678,922\n",
       "│    └─EnsNetFCSN (9)                                   [64, 800]            [64, 10]             --\n",
       "│    │    └─Sequential (subnet)                         [64, 800]            [64, 10]             678,922\n",
       "===================================================================================================================\n",
       "Total params: 35,865,950\n",
       "Trainable params: 35,865,950\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 63.02\n",
       "===================================================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 400.43\n",
       "Params size (MB): 143.46\n",
       "Estimated Total Size (MB): 544.10\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensnet = EnsNet(num_subnets = 10)\n",
    "X_dummy = torch.rand(64, 1, 28, 28) # Mimic MNIST batch size\n",
    "\n",
    "# ensnet.eval()\n",
    "# with torch.inference_mode():\n",
    "#     pred_labels = ensnet.predict(X_dummy)\n",
    "    \n",
    "summary(model = ensnet,\n",
    "        input_size = X_dummy.shape,\n",
    "        col_names = ['input_size', 'output_size', 'num_params'],\n",
    "        col_width = 20,\n",
    "        row_settings = ['var_names'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
